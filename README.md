# VLM_using_LLM
🧠 Vision-Language Question Answering using LLM
This project implements a Vision-Language Model (VLM) that combines computer vision and natural language understanding to perform Visual Question Answering (VQA). The model takes an image and a question as input and generates an answer using a combination of a CNN/Vision Transformer for visual features and BERT (a Large Language Model) for language encoding.

📌 Features

🔍 Multimodal Fusion of vision and language features.

🖼️ Supports image inputs and text questions.

🤖 Uses BERT for natural language question understanding.

🔗 Flexible model integration (ResNet, ViT, etc. for vision).

📈 Trainable and extensible for VQA tasks on datasets like DAQUAR, VQA v2.

📊 Evaluation Metric

BLEU Score

📚 Dataset

Use a dataset that provides:

Images

Questions (text)

Ground truth answers

Ex:Visual Question Answering- Computer Vision & NLP
