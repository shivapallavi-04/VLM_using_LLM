# VLM_using_LLM
ğŸ§  Vision-Language Question Answering using LLM
This project implements a Vision-Language Model (VLM) that combines computer vision and natural language understanding to perform Visual Question Answering (VQA). The model takes an image and a question as input and generates an answer using a combination of a CNN/Vision Transformer for visual features and BERT (a Large Language Model) for language encoding.

ğŸ“Œ Features

ğŸ” Multimodal Fusion of vision and language features.

ğŸ–¼ï¸ Supports image inputs and text questions.

ğŸ¤– Uses BERT for natural language question understanding.

ğŸ”— Flexible model integration (ResNet, ViT, etc. for vision).

ğŸ“ˆ Trainable and extensible for VQA tasks on datasets like DAQUAR, VQA v2.

ğŸ“Š Evaluation Metric

BLEU Score

ğŸ“š Dataset

Use a dataset that provides:

Images

Questions (text)

Ground truth answers

Ex:Visual Question Answering- Computer Vision & NLP
